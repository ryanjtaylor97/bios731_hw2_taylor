---
title: "BIOS 731 HW 2: Simulation Studies"
format: 
  pdf:
    number-depth: 4
    mainfont: Times New Roman
    include-in-header:
      - text: |
          \usepackage{makecell}
          \usepackage{longtable}
          \usepackage{mathrsfs}
    df-print: kable
    knitr:
      opts_chunk:
        echo: true,
        include: true
        message: false
        warning: false
        results: markup
        fig.align: center
        fig.height: 5
        fig.width: 7
        fig.path: "../results/"
        R.options:
          scipen: 999
          knitr.kable.NA: ""
editor: source
---

```{r}
#| label: setup
#| include: false

# Clear memory
rm(list = ls()); gc()

# Load packages
if(!require("here")) install.packages("here")
if(!require("pacman")) install.packages("pacman")
pacman::p_load(
  
  gtools, tictoc, parallel, doParallel, foreach,
  tidyverse, magrittr,
  knitr, kableExtra, broom
)

# Switch to determine whether to run the simulation step
DO_SIMULATIONS <- T
```

```{r}
#| label: load functions

source(here::here("source", "01_simulate_data.R"))
source(here::here("source", "02_apply_methods.R"))
source(here::here("source", "03_extract_estimates.R"))

# Define simulation function to parallelize
run_sim_scenario <- function(index){
  
  # Define row from parameters in global environment
  p <<- index
  
  # Load functions
  source(here::here("source", "01_simulate_data.R"))
  source(here::here("source", "02_apply_methods.R"))
  source(here::here("source", "03_extract_estimates.R"))
  
  # Run simulation process
  source(here::here("simulations", "run_simulations.R"))
  
  # Save results
  return(list("results" = results_df,
              "timing_log" = timing))
}
```


# Problem 1.1: ADEMP Structure

* *A*: The goal of this simulation study is to evaluate the accuracy of a linear regression to estimate a binary treatment effect. We also want to compare the coverage and computational efficiency of three methods of estimating a 95\% confidence interval around the treatment effect estimates: a Wald confidence interval, a bootstrap percentile interval, and a bootstrap $t$ interval. All of these methods should generate intervals that cover the true treatment effect in at least 95\% of simulations.

* *D*: We generate data under the assumption 
$$
Y_i = \beta_0 + \beta_{treat} X_i + \epsilon_i
$$

for $i = 1, \ldots, n$.

In all scenarios, $X_i$ is a binary variable that we randomly generate with $P(X_i = 1) = 0.5$, and we set $\beta_0 = 1$.

We set up a full factorial simulation design with the alternatives:

$n = 10, \ 50, \text{ or } 500$;

$\beta_{treat} = 0, \ 0.5, \text{ or } 2$;

$\epsilon_i {\overset{iid}{\sim}} N(0,2) \text{ or } \epsilon_i = u * \sqrt{2*\frac{\nu - 2}{\nu}}, \text{ where } u {\overset{iid}{\sim}} t_\nu \text{ and } \nu = 3$.

Therefore, we include **18** total scenarios.

* *E*: We are trying to learn about one estimand through this study: $\hat{\beta}_{treat}$ from a linear regression.

* *M*: We are evaluating one method for a point estimate $\hat{\beta}_{treat}$, which is linear regression. We are then comparing three methods for a confidence interval around this estimand: a Wald confidence interval, a bootstrap percentile confidence interval, and a bootstrap $t$ confidence interval.

* *P*: We will evaluate $\hat{\beta}_{treat}$ using its bias. We will compare the three methods for the confidence interval based on coverage and computation speed.


# Problem 1.2: nSim

```{r}
#| label: calculate simulations

# Define desired coverage and max Monte Carlo error
ci_pct <- 0.95
mcse_max <- 0.01

# Calculate necessary simulations
nSim <- ci_pct * (1 - ci_pct) / (mcse_max)^2
```

Based on desired coverage of 95\% with a maximum Monte Carlo standard error of 1\%, we should perform `r nSim` simulations for each scenario.

# Problem 1.3: Implementation

First, we set up the full factorial design, defining all of the parameters that change between scenarios and expanding them into a data frame of all combinations. We also define the number of outer and inner bootstrap resamples we will run. We use the recommended numbers from the homework: 500 outer bootstrap resamples and 100 inner bootstrap resamples.

```{r}
#| label: define parameters

# Define sample size, treatment effect, error distribution
data_n <- c(10, 50, 100)
beta_true <- c(0, 0.5, 2)
error_dist <- c("gauss", "t")

# Combine these into table
params <- expand_grid(data_n, beta_true, error_dist)

# Set bootstrap numbers
boot_outer <- 500
boot_inner <- 100
```

Next, we define the seeds we will use for these simulations. We use a different seed for each simulation and for each outer bootstrap resample in each scenario. We generate these seeds by setting one seed arbitrarily, then randomly generating numbers from 1 to the total number of seeds we need. Then, we organize these into a nested list by scenario and by simulation number.

```{r}
#| label: define seeds
### Define seeds

# Count number of randomization steps we will perform
# -> For each scenario: n simulations, 1 data set, B_outer bootstrap samples per data set, B_inner resamples per outer sample
num_sims <- nrow(params) * nSim * (1 + boot_outer * boot_inner)

# -> We will use 1 seed per bootstrap
num_seeds <- nrow(params) * nSim * (1 + boot_outer)

# Set initial seed, outside of next range
set.seed(num_seeds + 413)

# Randomize order of other seeds
seeds <- sample(1:num_seeds, num_seeds)

# Organize these by simulation step
seed_list <- split(
  seeds, 
  cut(1:num_seeds, breaks = nrow(params))
) %>% 
  map(~split(.x, cut(1:length(.x), breaks = nSim)))
```

Then, before running these simulations, we try to improve the computational efficiency of our process. For both bootstrap methods of generating a confidence interval, we do not need the estimated standard error of our estimated $\beta_{treat}$; we only need the coefficient itself. While this is most easily generated by the function $\texttt{lm}$, it can also be calculated using the function $\texttt{lm.fit}$. We check that these two methods return the same coefficient, and also use $\texttt{benchmark}$ to test their computation speeds against each other. We find that $\texttt{lm.fit}$ is significantly quicker, so we use it in both of our bootstrap functions.

```{r}
#| label: test model fit time

set.seed(num_seeds + 978)

test_df <- get_simdata(n = max(data_n),
                       beta_treat = max(beta_true),
                       variance = 2,
                       error_dist = "gauss")

tibble(
  lm = coef(fit_model_lm(test_df)),
  lm.fit = coef(fit_model_lm_fit(x_mat = matrix(cbind(1, test_df$x), 
                                             nrow = nrow(test_df)), 
                              y_vec = test_df$y))
)

bench <- microbenchmark::microbenchmark(
  "lm" = coef(fit_model_lm(test_df))[2],
  "coef_lm.fit" =  coef(fit_model_lm_fit(x_mat = matrix(cbind(1, test_df$x), 
                                                     nrow = nrow(test_df)), 
                                      y_vec = test_df$y))[2]
  )

bench
```

Finally, we run simulations for all 18 scenarios listed. 

```{r}
#| label: run simulations

# Determine number of cores available
num_cores <- detectCores() - 2

if(DO_SIMULATIONS){
  
  # Make cluster for parallelization
  cl <- makeCluster(num_cores)
  
  registerDoParallel(cl)
  
  # Combine results and timing from all scenarios
  simulation_output <- foreach(
    i = 1:nrow(params), 
    .packages = c("tidyverse", "magrittr", "tictoc", "broom"),
    .inorder = F
  ) %dopar% {
    
    # Move all necessary variables to global environment
    .GlobalEnv$params <- params
    .GlobalEnv$nSim <- nSim
    .GlobalEnv$seed_list <- seed_list
    .GlobalEnv$boot_outer <- boot_outer
    .GlobalEnv$boot_inner <- boot_inner
    
    run_sim_scenario(index = i)
  }
  
  stopCluster(cl)
  
  # Save combined output
  save(simulation_output, 
       file = here::here("data", "combined_scenario_output.rda"))
  
} else {
  
  # Load output
  load(file = here::here("data", "combined_scenario_output.rda"))
}
```

